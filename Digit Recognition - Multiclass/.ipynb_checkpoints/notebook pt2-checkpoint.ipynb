{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0b8582-ff70-44a0-bb40-9f34a4a2a7ef",
   "metadata": {},
   "source": [
    "# About\n",
    "In this notebook, we will:\n",
    "- Test our model from last time on unseen data\n",
    "- Investigate whether a convolutional neuron network can outperform the original neural network comprised of Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf749e9-37e2-46c3-8701-d820170452ed",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b43066-d7c0-422d-9942-883df0160d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea874c9b-6eef-49fa-a8ef-182631d123c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducable results\n",
    "random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee5df0-67ee-401b-8b05-9a57698c9cf8",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7656fea6-1e48-481c-8d7a-e4a715f88e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/X.npy')\n",
    "y = np.load('data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd635e8-cd4c-494f-8d1e-0b842387dec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has a shape of (5000, 400)\n",
      "y has a shape of (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'X has a shape of {X.shape}')\n",
    "print(f'y has a shape of {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b10de-c0b2-4ce2-9a59-5ab2d8471bd6",
   "metadata": {},
   "source": [
    "## Load model\n",
    "We will load the same model we trained from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7044e0-0c2e-47e7-b256-286342698428",
   "metadata": {},
   "outputs": [],
   "source": [
    "overfitted_model = load_model('models/Denselayer.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9469b353-e588-4a28-aeb4-3e6bbc0ce53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392 out of 400 digits correctly predicted \n",
      "98.0% success rate\n"
     ]
    }
   ],
   "source": [
    "m,n = X.shape\n",
    "indices = np.random.randint(0,m,n)\n",
    "num_correct_predictions = sum([np.argmax(overfitted_model.predict(X[idx].reshape(1,n), verbose=0)) for idx in indices]==y[indices].reshape(-1,))\n",
    "\n",
    "print(f'{num_correct_predictions} out of {len(indices)} digits correctly predicted \\n{num_correct_predictions/len(indices) * 100:.1f}% success rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a2c38-ac2d-40a2-b029-ecd6e44f76f0",
   "metadata": {},
   "source": [
    "This model does extremely good at predicting target values for data which it trained on. It may have \"overfitted the data\". <br>Let's see what happens when we train the same model as last time but test it on a subset of data it has not seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa5d2d-1188-42e6-a3c0-9b1001fb2113",
   "metadata": {},
   "source": [
    "## Retrain Model\n",
    "We will load the same model, before it was trained (with randomly initialised weights), from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6436d00b-1430-43a1-a4b2-c62283214309",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/Denselayer_beforetraining.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c9b7bd-0f5a-4617-bbf9-873a206301fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need to run pip install scikit-learn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21b3664-017e-4253-ad1d-78f3aeae588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has shape (4000, 400)\n",
      "y_train has shape (4000, 1)\n",
      "X_test has shape (1000, 400)\n",
      "y_test has shape (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# do a 80|20 training|test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "print(f'X_train has shape {X_train.shape}')\n",
    "print(f'y_train has shape {y_train.shape}')\n",
    "print(f'X_test has shape {X_test.shape}')\n",
    "print(f'y_test has shape {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52292332-17e4-4978-9881-b2f78654579e",
   "metadata": {},
   "source": [
    "We have split our data into two subsets: training and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098f3d5a-408a-4709-a8ce-20bf161656c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.0538\n",
      "Epoch 2/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9513\n",
      "Epoch 3/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5479\n",
      "Epoch 4/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3858\n",
      "Epoch 5/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3135\n",
      "Epoch 6/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2746\n",
      "Epoch 7/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2479\n",
      "Epoch 8/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2274\n",
      "Epoch 9/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2110\n",
      "Epoch 10/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1963\n",
      "Epoch 11/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1833\n",
      "Epoch 12/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1719\n",
      "Epoch 13/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1615\n",
      "Epoch 14/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1516\n",
      "Epoch 15/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1423\n",
      "Epoch 16/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1322\n",
      "Epoch 17/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1233\n",
      "Epoch 18/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1148\n",
      "Epoch 19/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1074\n",
      "Epoch 20/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0997\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "model.compile(\n",
    "    loss = SparseCategoricalCrossentropy(from_logits = True),\n",
    "    optimizer = Adam(0.001)\n",
    ")\n",
    "\n",
    "# train model\n",
    "num_epochs = 20\n",
    "history = model.fit(X_train, y_train,epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70eb5960-35a8-4d56-83d6-10b49a2498e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 out of 400 digits correctly predicted \n",
      "92.5% success rate\n"
     ]
    }
   ],
   "source": [
    "m,n = X_test.shape\n",
    "indices = np.random.randint(0,m,n)\n",
    "num_correct_predictions = sum([np.argmax(model.predict(X_test[idx].reshape(1,n), verbose=0)) for idx in indices]==y_test[indices].reshape(-1,))\n",
    "\n",
    "print(f'{num_correct_predictions} out of {len(indices)} digits correctly predicted \\n{num_correct_predictions/len(indices)*100:.1f}% success rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31419e6c-d858-49bc-b47e-6c4f55b02168",
   "metadata": {},
   "source": [
    "Still convincing, but the fact that the prediction rate has come down suggests our model is overfitting to the training data a little bit. Let's explore convolutional neural networks, which are less prone to overfitting (I'll explain later). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5099f-a4e0-4aea-bcdd-99b056244aaa",
   "metadata": {},
   "source": [
    "# Understanding Convolutional Neuron Networks\n",
    "A convolutional neuron network responds to inputs differently than the neural network we built from the last notebook. I found the article from Axel Thevenot has useful animations to explain: https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148\n",
    "![cnn](media/cnn.gif) <br>\n",
    "Let's break it down:\n",
    "- On the left, we have our 9×9 image, a total of 81 pixels\n",
    "- The 3×3 grid in the middle represents our neuron in the convolutional neuron network (CNN). Neurons in a CNN are often referred to as kernels\n",
    "- On the right is the kernel's output\n",
    "\n",
    "Still with me, you're doing great! Instead of consuming the entire input image in one go, as did our neurons in the Dense layer, our kernel slides over the image. Let's see what the kernel does in step 1 <br>\n",
    "## Kernel Calculation\n",
    "![cnn_step1](media/cnn_step1.jpg)<br><br>\n",
    "Those 9 pixels on the left serve as input to our kernel - each pixel is represented by a number between 0-255 (if you don't know about this, research greyscale values).<br>\n",
    "The kernel itself has 9 weights because 3×3=9.<br><br>\n",
    "![kernel](media/kernel.jpg)<br><br>\n",
    "If we perform element-wise multiplication & sum the results, we get a single number ouput.<br><br>\n",
    "![kernel](media/kernel_calculation.jpg) <br><br>\n",
    "This is the number representing the top left most pixel in the output. Repeat this calculation every time the kernel slides across."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77999c6f-2929-4654-ba78-283ca51b6e11",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "Let's assume the output of our kernel is the following 7×7 matrix <br><br>\n",
    "![kernel_output_before_activation](media/kernel_output_before_activation.jpg) <br><br>\n",
    "We are not done, each kernel has an activation function - here we will use Rectified Linear Unit (ReLU). That means anything negative turns to zero and anything else stays the same. <br> <br>\n",
    "![kernel_output_before_activation](media/kernel_output_after_activation.jpg) <br><br>\n",
    "This 2D matrix is the output of 1 convolutional neuron. So after all that, how is this any better than the traditional dense neural network. Interestingly, this output can represent 49 pixels or a 7×7 pixel grid. If we plot this as a greyscale image, we can see what features of the input image our neurons are focusing on. I won't plot the output since all numbers here are fictional, but if you're interested in what each neuron actually sees, I recommend: https://medium.com/@neethamadhu.ma/visualizing-the-magic-a-guide-to-understanding-convolutional-neural-networks-c701978373c0 <p>\n",
    "So how are these networks less prone to overfitting? I urge you think about the number of weights in your dense neural network (DNN) vs convolutional neural network (CNN). In a DNN, for each neuron, there is a weight per pixel. In a CNN, for each neuron, there are only 9 weights full stop. So in the case of a 20×20 image, we will have 400 weights for the DNN neuron vs. 9 weights for the CNN neuron. Less weights mean the network stays generalisable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a53cc0-997e-48c7-a927-64d4d9f3d49d",
   "metadata": {},
   "source": [
    "# Applying Convolutional Neuron Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c76623-891c-4f85-95e2-f3e45dfed64b",
   "metadata": {},
   "source": [
    "The following image depicts what our neural network is doing to the original 20×20 greyscale digit image <br>\n",
    "![cnn_architecture](media/cnn_architecture.png) <br>\n",
    "Image generated with Python. Source code can be found at: https://github.com/gwding/draw_convnet <p>\n",
    "Notice there are 10 outputs, this corresponds to numbers between 0-9, the 10 digits our handwritten digit could be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224dd22-8f6d-49bf-93e1-1402aebc4b10",
   "metadata": {},
   "source": [
    "## Build and Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c7044d-bc4d-4d5a-a1c9-b5c265125532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,394</span> (220.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,394\u001b[0m (220.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,394</span> (220.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,394\u001b[0m (220.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build network \n",
    "cnn = Sequential(\n",
    "    [\n",
    "        Input(shape = (20,20,1)),\n",
    "        Conv2D(filters = 32, kernel_size=(3,3), activation = 'relu'),\n",
    "        MaxPool2D(pool_size = (2,2)),\n",
    "        Conv2D(filters = 64, kernel_size=(3,3), activation = 'relu'),\n",
    "        MaxPool2D(pool_size = (2,2)),\n",
    "        Flatten(),\n",
    "        Dense(64, activation = 'relu'),\n",
    "        Dense(10)\n",
    "    ]\n",
    ")\n",
    "cnn.save('models/cnn_beforetraining.keras')\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d8d841-3fc2-4b30-830c-48250afc6369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently, our inputs in X_train are numpy arrays of shape (400,). Before we train our model, we need to reshape the inputs to shape (20, 20, 1) \n",
      "as this is what the CNN is expecting.\n"
     ]
    }
   ],
   "source": [
    "print(f'Currently, our inputs in X_train are numpy arrays of shape {X_train[0].shape}. Before we train our model, we need to reshape the inputs to shape {cnn.layers[0].input.shape[1:]} \\nas this is what the CNN is expecting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc377d7-5f87-417c-849e-9b132723aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old inputs shape: (4000, 400)\n",
      "New inputs shape: (4000, 20, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X_traincnn = []\n",
    "\n",
    "for x in X_train:\n",
    "    x = x.reshape(20,20).T.reshape(20,20,1) # included a transpose to get our pixels the right way around\n",
    "    X_traincnn.append(x)\n",
    "\n",
    "X_traincnn = np.array(X_traincnn)\n",
    "y_traincnn = y_train\n",
    "print(f'Old inputs shape: {X_train.shape}')\n",
    "print(f'New inputs shape: {X_traincnn.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d21ebc07-f30b-4198-9308-4ec9c75061cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.5351\n",
      "Epoch 2/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2487\n",
      "Epoch 3/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1495\n",
      "Epoch 4/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1115\n",
      "Epoch 5/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0881\n",
      "Epoch 6/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0685\n",
      "Epoch 7/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0516\n",
      "Epoch 8/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0384\n",
      "Epoch 9/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0332\n",
      "Epoch 10/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0250\n",
      "Epoch 11/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0227\n",
      "Epoch 12/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0192\n",
      "Epoch 13/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0208\n",
      "Epoch 14/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0253\n",
      "Epoch 15/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0138\n",
      "Epoch 16/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0131\n",
      "Epoch 17/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0079\n",
      "Epoch 18/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0089\n",
      "Epoch 19/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0066\n",
      "Epoch 20/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25e17489990>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define loss\n",
    "cnn.compile(\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer =Adam(0.001)\n",
    ")\n",
    "\n",
    "# train model\n",
    "cnn.fit(X_traincnn, y_traincnn, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd682e1-6696-4ab6-bfd5-75715ef99f8c",
   "metadata": {},
   "source": [
    "## Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "317688fd-5a8f-4ea4-91e8-501d150db8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test data inputs\n",
    "X_testcnn = []\n",
    "\n",
    "for x in X_test:\n",
    "    x = x.reshape(20,20).T.reshape(20,20,1) # included a transpose to get our pixels the right way around\n",
    "    X_testcnn.append(x)\n",
    "\n",
    "X_testcnn = np.array(X_testcnn)\n",
    "y_testcnn = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af2e6c79-30f9-47e1-b8cb-b2427cf2251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feedforward data\n",
    "outputs = cnn.predict(X_testcnn, verbose = 0)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e17402c7-09d5-45db-b443-2dbf1e529b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963 out of 1000 digits correctly predicted \n",
      "96.3% success rate\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "m,n = outputs.shape\n",
    "predictions = [np.argmax(outputs[i]) for i in range(m)]\n",
    "num_correct_predictions = sum([predictions[i] == y_testcnn[i] for i in range(m)])\n",
    "print(f'{num_correct_predictions[0]} out of {m} digits correctly predicted \\n{num_correct_predictions[0]/m *100:.1f}% success rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e1ade-5494-4e79-9c7e-9674798a0736",
   "metadata": {},
   "source": [
    "Interesting. The CNN does 4-5% better than the dense neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba83538-6e4d-4eb4-ad53-ffe27a682f48",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "The above networks have been trained for only 20 epochs, which is very little. Let us see what happens when we train for 200 epochs, which shouldn't take much longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9080d-0058-431c-8e18-3f5357ee209e",
   "metadata": {},
   "source": [
    "## Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd00f4fa-4e95-4eb5-98ed-782163b32173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/Denselayer_beforetraining.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5220a45-9f05-4461-9c19-62b8c771a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss & optimizer\n",
    "model.compile(\n",
    "    loss = SparseCategoricalCrossentropy(from_logits = True),\n",
    "    optimizer = Adam(0.001)\n",
    ")\n",
    "\n",
    "# train model\n",
    "num_epochs = 200\n",
    "history = model.fit(X_train, y_train,epochs = num_epochs, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c68b43a-def5-44ac-b131-fc718b0ce368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921 out of 1000 digits correctly predicted \n",
      "92.1% success rate\n"
     ]
    }
   ],
   "source": [
    "# feedforward data\n",
    "outputs = model.predict(X_test, verbose = 0)\n",
    "\n",
    "# metrics\n",
    "m,n = outputs.shape\n",
    "predictions = [np.argmax(outputs[i]) for i in range(m)]\n",
    "num_correct_predictions = sum([predictions[i] == y_test[i] for i in range(m)])\n",
    "success_rate_dnn = num_correct_predictions[0]/m *100\n",
    "\n",
    "print(f'{num_correct_predictions[0]} out of {m} digits correctly predicted \\n{success_rate_dnn:.1f}% success rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423212af-74e5-4e91-a05d-94581052689d",
   "metadata": {},
   "source": [
    "Performs roughly the same as its 20 epoch counterpart ~ 90.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3284106-6424-4fe9-87ff-e363911ff19f",
   "metadata": {},
   "source": [
    "## Convolutional Neuron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d419f382-75e1-4a33-b5dc-9eef7a1fdec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = load_model('models/cnn_beforetraining.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3950336c-4052-43e2-899b-7a7376f5c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss & optimizer\n",
    "cnn.compile(\n",
    "    loss = SparseCategoricalCrossentropy(from_logits = True),\n",
    "    optimizer = Adam(0.001)\n",
    ")\n",
    "\n",
    "# train model\n",
    "num_epochs = 200\n",
    "history = cnn.fit(X_traincnn, y_traincnn,epochs = num_epochs, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29e14a4b-2692-4c4e-aecf-a405e29225b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973 out of 1000 digits correctly predicted \n",
      "97.3% success rate\n"
     ]
    }
   ],
   "source": [
    "# feedforward data\n",
    "outputs = cnn.predict(X_testcnn, verbose = 0)\n",
    "\n",
    "# metrics\n",
    "m,n = outputs.shape\n",
    "predictions = [np.argmax(outputs[i]) for i in range(m)]\n",
    "num_correct_predictions = sum([predictions[i] == y_testcnn[i] for i in range(m)])\n",
    "success_rate_cnn = num_correct_predictions[0]/m *100\n",
    "\n",
    "print(f'{num_correct_predictions[0]} out of {m} digits correctly predicted \\n{success_rate_cnn:.1f}% success rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01ce0e2d-9bbe-4531-ba74-ae04d6ae6e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN success rate is better than DNN by 5.2%. This was tested on 1000 unseen images.\n"
     ]
    }
   ],
   "source": [
    "print(f'CNN success rate is better than DNN by {success_rate_cnn - success_rate_dnn:.1f}%. This was tested on {len(X_test)} unseen images.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
